\documentclass[12pt,letterpaper,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage{minted}
\usepackage[table]{xcolor}

\begin{document}

\title{CS 491-004 Project 2 Report}
\author{Sean Gillen}
\date{February 25, 2019}
\maketitle

\section*{Introduction}
In this project, I calculated, tested, and compared performance of the six simple triple-loop and the six blocked-version matrix multiplication algorithms on the Pantarhei cluster at the University of Alabama.

\section*{Part 1}
In both $ijk$ and $jik$ multiplication, the inner loop accesses \texttt{a[i*n+k]} and \texttt{b[k*n+j]}.
For large values of $n$, since a line of the cache holds 10 doubles and elements of \texttt{a} are read in consecutive order, there will be a read cache miss from matrix $A$ every 10 iterations (0.1 per iteration),
and each read from matrix $B$ results in a compulsory miss. Matrix $C$ is never read in this algorithm, so there are a total of $0.1n^3$ misses reading matrix $A$, $n^3$ misses reading matrix $B$, and 0 misses reading matrix $C$.

In $kij$ and $ikj$ multiplication, the inner loop accesses \texttt{c[i*n+j]} and \texttt{b[k*n+j]} and the middle loop accesses \texttt{a[i*n+k]}.
The reads from matrices $B$ and $C$ will both miss once every 10 iterations of the inner loop. For large $n$, all lines of the cache are used by the inner loop so the read from matrix $A$ will miss at every iteration.
This means that in most cases there are $n^2$ misses reading matrix $A$, $0.1n^3$ misses reading matrix $B$, and $0.1n^3$ misses reading matrix $C$.

Finally, in $jki$ and $kji$ multiplication, the inner loop accesses \texttt{c[i*n+j]} and \texttt{a[i*n+k]} and the middle loop accesses \texttt{b[k*n+j]}.
The reads from matrices $A$ and $C$ in the inner loop are both columnwise, so miss at every iteration for large $n$.
The read from matrix $B$ also misses at every iteration of the middle loop when $n$ is sufficiently large for the inner loop to exhaust the cache.
As such, for large $n$, both $jki$ and $kji$ multiplication will result in $n^3$ misses reading matrix $A$, $n^2$ misses reading matrix $B$, and $n^3$ misses reading matrix $C$.

For small $n$, regardless of which algorithm is used, each value will be read only once since the cache is never filled and values read at previous iterations of each loop are never replaced.
For $ikj$, $jki$, $kij$, and $kji$ multiplication, the total size of matrices $A$, $B$, and $C$ must not exceed the size of the cache ($3n^2\le60\times10\Rightarrow n<14$) for this property to have effect.
For $ijk$ and $jik$ multiplication, since values from matrix $C$ are not stored in the cache, the total size of matrices $A$ and $B$ must not exceed the size of the cache ($2n^2\le60\times10\Rightarrow n<17$).

\begin{center}
	\subsection*{$n=10000$}
	\begin{tabular}{c | c c c c | c c}
		& \multicolumn{4}{c|}{Misses} &  &  \\
		& $A$ & $B$ & $C$ & Total & Total Reads & Miss Rate \\
		\hline
		$ijk$ & $10^{11}$ & $10^{12}$ & 0 & $1.1\times10^{12}$ & $2\times10^{12}$ & 55\% \\
		$ikj$ & $10^8$ & $10^{11}$ & $10^{11}$ & $2.001\times10^{11}$ & $2.0001\times10^{12}$ & 10\% \\
		$jik$ & $10^{11}$ & $10^{12}$ & 0 & $1.1\times10^{12}$ & $2\times10^{12}$ & 55\% \\
		$jki$ & $10^8$ & $10^{12}$ & $10^{12}$ & $2.0001\times10^{12}$ & $2.0001\times10^{12}$ & 100\% \\
		$kij$ & $10^8$ & $10^{11}$ & $10^{11}$ & $2.001\times10^{11}$ & $2.0001\times10^{12}$ & 10\% \\
		$kji$ & $10^8$ & $10^{12}$ & $10^{12}$ & $2.0001\times10^{12}$ & $2.0001\times10^{12}$ & 100\% 
	\end{tabular}
	\subsection*{$n=10$}
	\begin{tabular}{c | c c c c | c c}
		& \multicolumn{4}{c|}{Misses} &  &  \\
		& $A$ & $B$ & $C$ & Total & Total Reads & Miss Rate \\
		\hline
		$ijk$ & 10 & 10 & 0 & 20 & 2000 & 1.0\% \\
		$ikj$ & 10 & 10 & 10 & 30 & 2100 & 1.4\% \\
		$jik$ & 10 & 10 & 0 & 20 & 2000 & 1.0\% \\
		$jki$ & 10 & 10 & 10 & 30 & 2100 & 1.4\% \\
		$kij$ & 10 & 10 & 10 & 30 & 2100 & 1.4\% \\
		$kji$ & 10 & 10 & 10 & 30 & 2100 & 1.4\% \\
	\end{tabular}
\end{center}

\newpage
\section*{Part 2}
In the blocked matrix multiplication algorithm with $B=10$, the inner three loops will result in as many read cache misses as noted in Part 1 for simple matrix multiplication with $n=10$.
In blocked matrix multiplication, we cannot skip reading from matrix $C$ with $ijk$ and $jik$ multiplication, so in all cases the submatrix multiplication results in 30 cache misses.
Since the values in each block do not overlap, this many read cache misses will result from each iteration of the third loop---that is, $\lfloor \frac{n}{B}\rfloor^3$ times.\footnote
{When $n\ \textrm{mod}\ B\ne0$, the third loop will execute $\lceil\frac{n}{B}\rceil^3$ times but $\lceil\frac{n}{B}\rceil^3-\lfloor\frac{n}{B}\rfloor^3$ of these iterations will result in fewer read cache misses---as many as simple matrix multiplication with matrix dimension $n\ \textrm{mod}\ B$.}

With $n=10000$, blocked matrix multiplication with $B=10$ will result in $(\frac{10000}{10})^2=10^6$ blocks, each resulting in 30 read cache misses as noted above.

\begin{center}
	\subsection*{$n=10000$}
	\begin{tabular}{c | c c c c | c c}
		& \multicolumn{4}{c|}{Misses} &  & \\
		& $A$ & $B$ & $C$ & Total & Total Reads & Miss Rate \\
		\hline
		$ijk$ & $10^7$ & $10^7$ & $10^7$ & $3\times10^7$ & $2.1\times10^9$ & 1.4\% \\
		$ikj$ & $10^7$ & $10^7$ & $10^7$ & $3\times10^7$ & $2.1\times10^9$ & 1.4\% \\
		$jik$ & $10^7$ & $10^7$ & $10^7$ & $3\times10^7$ & $2.1\times10^9$ & 1.4\% \\
		$jki$ & $10^7$ & $10^7$ & $10^7$ & $3\times10^7$ & $2.1\times10^9$ & 1.4\% \\
		$kij$ & $10^7$ & $10^7$ & $10^7$ & $3\times10^7$ & $2.1\times10^9$ & 1.4\% \\
		$kji$ & $10^7$ & $10^7$ & $10^7$ & $3\times10^7$ & $2.1\times10^9$ & 1.4\% \\
	\end{tabular}
\end{center}

\newpage
\section*{Part 3}
The measured execution times of the algorithms in Part 1 (block size 1) and Part 2 (block size $B$) on the Pantarhei cluster multiplying randomly-generated $2048\times2048$ matrices of doubles are given below.

\begin{center}
	\subsection*{$n=2048$}
	\begin{tabular}{c | r | r r r r r}
		& & \multicolumn{5}{c}{Block size $B$} \\
		& \multicolumn{1}{c |}{No blocking} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{16} & \multicolumn{1}{c}{32} & \multicolumn{1}{c}{64} & \multicolumn{1}{c}{128} \\
		\hline
		$ijk$ & 243.4 s & 44.4 s & 42.3 s & 43.1 s & 42.2 s & 110.0 s \\
		$ikj$ & 72.9 s & 36.8 s & 35.1 s & 36.1 s & 33.8 s & 35.1 s \\
		$jik$ & 226.4 s & 45.3 s & 42.9 s & 43.3 s & 42.9 s & 84.5 s \\ 
		$jki$ & 321.0 s & 49.9 s & 49.4 s & 51.1 s & 61.4 s & 113.6 s \\
		$kij$ & 71.4 s & 36.9 s & 35.3 s & 36.0 s & 33.9 s & 35.9 s \\
		$kji$ & 350.2 s & 49.5 s & 47.4 s & 50.9 s & 55.4 s & 98.1 s
	\end{tabular}
\end{center}

My implementation's correctness was verified by calculating the maximum difference between values in the matrix resulting from $ijk$ multiplication and the corresponding valuees in the matrices resulting from the other multiplication algorithms for each block size tested. The maximum difference obtained was the expected 0.000000.

The code was compiled on Pantarhei with gcc 7.3.0 with no optimization flags. The increase in runtime between $B=64$ and $B=128$ suggests the optimal block size is 64 for each algorithm except $jki$ and $kji$, which appear to perform best with a block size of 16. As these algorithms are the slowest anyway, subsequent testing was performed only with $B=64$.

The significantly faster runtimes for $ikj$ and $kij$ multiplication on the cluster may be explained by a greater cache line size than assumed in Part 2 causing the caching of one block to also cache some values in the next block.

\newpage
\section*{Part 4}
I added $2\times2$ register blocking (as in Project 1) to each algorithm from Part 2 and tested their runtimes with $n=2048$ and $B=64$ on the Pantarhei cluster after compiling them with different compilers and optimization settings. The measured performance of each combination is listed below.
\begin{center}
	\subsection*{$n=2048$, $B=64$}
	\begin{tabular}{c | r r r r | r r r r}
		& \multicolumn{4}{c|}{gcc 7.3.0} & \multicolumn{4}{c}{gcc 5.4.0} \\
		& \texttt{-O0} & \texttt{-O1} & \texttt{-O2} & \texttt{-O3} & \texttt{-O0} & \texttt{-O1} & \texttt{-O2} & \texttt{-O3} \\
		\hline
		$ijk$ & 12.5 s & 3.6 s & 3.4 s & 3.5 s & 12.1 s & 3.6 s & 3.5 s & 3.5 s \\
		$ikj$ & 12.9 s & 3.4 s & 2.6 s & \cellcolor{yellow!50} 2.2 s & 13.1 s & 3.4 s & 2.8 s & 2.7 s \\
		$jik$ & 12.8 s & 3.9 s & 4.0 s & 3.8 s & 12.2 s & 4.2 s & 3.7 s & 3.7 s \\ 
		$jki$ & 15.0 s & 16.0 s & 16.1 s & 16.1 s & 15.2 s & 16.1 s & 16.1 s & 16.1 s \\
		$kij$ & 13.1 s & 3.5 s & 2.9 s & 2.4 s & 13.3 s & 3.6 s & 2.9 s & 2.9 s \\
		$kji$ & 14.7 s & 15.4 s & 15.5 s & 15.5 s & 14.9 s & 15.5 s & 15.5 s & 15.5 s
	\end{tabular}
\end{center}

My implementation's correctness was verified by the same method as in Part 3. The maximum difference obtained was 0.000003; this small difference is easily attributable to floating point arithmetic imprecision.
The minimum runtime, 2.2 seconds, is highlighted in the table above.

As an example of the implementation of cache and register blocking, my implementation of $ijk$ multiplication is reproduced below. Implementations of the other algorithms are similar.
\begin{minted}[tabsize=2,fontsize=\small]{c}
void ijk(double *a, double *b, double *c, int n, int B) {
	for(int i = 0; i < n; i += B) {
		for(int j = 0; j < n; j += B) {
			for(int k = 0; k < n; k += B) {
				for(int i1 = i; i1 < i + B && i1 < n; i1 += 2) {
					for(int j1 = j; j1 < j + B && j1 < n; j1 += 2) {
						register double c00, c01, c10, c11;
						c00 = c01 = c10 = c11 = 0;
						for(int k1 = k; k1 < k + B && k1 < n; k1 += 2) {
							register double a00 = a[i1*n+k1];
							register double a10 = a[(i1+1)*n+k1];
							register double a01 = a[i1*n+k1+1];
							register double a11 = a[(i1+1)*n+k1+1];
							register double b00 = b[k1*n+j1];
							register double b10 = b[(k1+1)*n+j1];
							register double b01 = b[k1*n+j1+1];
							register double b11 = b[(k1+1)*n+j1+1];
							
							c00 += a00 * b00 + a01 * b10;
							c10 += a10 * b00 + a11 * b10;
							c01 += a00 * b01 + a01 * b11;
							c11 += a10 * a01 + a11 * b11;
						}
						c[i1*n+j1] += c00;
						c[(i1+1)*n+j1] += c10;
						c[i1*n+j1+1] += c01;
						c[(i1+1)*n+j1+1] += c11;
					}
				}
			}
		}
	}
}
\end{minted}

\section*{Conclusion}
The differences in efficiency between loop orders in matrix multiplication can be greatly reduced (from 390\% to 46\% in my tests with $n=2048$)
by blocking the matrices. However, the addition of compiler optimizations again amplifies these differences. At the highest optimization level of gcc 7.3.0,
the runtime for $ikj$ multiplication of $2048\times2048$ matrices on the Pantarhei cluster can be reduced
to 2.2 seconds, a speedup of 97.0\% over simple $ikj$ multiplication and 99.4\% over simple $kji$ multiplication, with the introduction of cache and register blocking.
 
\end{document} 