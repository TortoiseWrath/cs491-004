\documentclass[12pt,letterpaper,oneside]{article}
\usepackage[utf8]{inputenc}

\usepackage{minted}

\begin{document}

\title{CS 491-004 Project 1 Report}
\author{Sean Gillen}
\date{February 6, 2019}
\maketitle

\section*{Introduction}
In this project, I implemented and compared square matrix multiplication algorithms with
various levels of register reuse and tested the performance of these algorithms.

\section*{Part 1}
The implementation of \texttt{dgemm0} is reproduced below:
\inputminted{c}{dgemm0.c}
Note that with each iteration of the innermost loop, three memory accesses are made to retrieve the values
\texttt{a[i*n+k]}, \texttt{b[k*n+j]}, and \texttt{c[i*n+j]}, two floating-point operations are performed, and 
one memory access is made to store the result in \texttt{c[i*n+j]}.

Assuming that both floating-point operations can be completed in one cycle and each memory access adds a delay
of 100 cycles, each iteration of the inner loop uses 401 cycles. For $n=1000$, the body of the inner loop
is executed $1000^3$ times, corresponding to a total of $4.01\times10^{11}$ processor cycles. 
At 2 GHz, this is equal to \fbox{200.5 seconds}.

The implementation of \texttt{dgemm1} is reproduced below:
\inputminted{c}{dgemm1.c}
In this algorithm, the innermost loop includes only two memory accesses, with the other two memory accesses taking
place only with every iteration of the second-level loop. Using the assumptions noted above, this means the innermost loop body completes in 201 cycles
and the body of the second-level loop completes in $201n+200$ cycles. For $n=1000$, the total execution time
of \texttt{dgemm1} is then $2.012\times10^{11}$ cycles or, at 2 GHz, \fbox{100.6 seconds}, $49.8\%$ faster than \texttt{dgemm0}.

On my computer, \texttt{dgemm1} completes up to $32\%$ faster than \texttt{dgemm0} 
on tests with $n\le1024$, but $1\%$ slower than \texttt{dgemm0} when $n=2048$ 
(see pages~\pageref{results0}--\pageref{results1} for complete results).

\section*{Part 2}
My implementation of \texttt{dgemm2} is reproduced below:
\inputminted{c}{dgemm2.c}
On my computer (see page~\pageref{specs} for system informattion), \texttt{dgemm2} completes 56--70\% faster than \texttt{dgemm0} 
(see page~\pageref{results2} for complete results).


 
\section*{Part 3}
I implemented \texttt{dgemm3} as follows:
\inputminted{c}{dgemm3.c}
On my computer, \texttt{dgemm3} completes up to 97\% faster than \texttt{dgemm0} 
(see page~\pageref{results3} for complete results).

\section*{Performance measurements}

I tested each matrix multiplication algorithm on an \label{specs} Intel Core i7-6700HQ processor running at 3.5 GHz after compiling
the following test program with \texttt{gcc} 6.3.0 at optimization level 3. The GFLOPS performance for each algorithm
is calculated based on an assumption that each algorithm includes $3n^3$ floating-point operations.

\inputminted{c}{timing.c}

The same tests I performed may be run with the attached source code using \texttt{make}. To compile without running the tests,
run \texttt{make compile}.
The results of these tests are listed below.
\begin{center}
    \subsection*{\texttt{dgemm0}}
    \label{results0}
    \begin{tabular}{r r r}
        $n$ & seconds & GFLOPS \\
        \hline
        64 & 0.001060 & 0.7419 \\
        128 & 0.011381 & 0.5528 \\
        256 & 0.045656 & 1.1024 \\
        512 & 0.371264 & 1.0845 \\
        1024 & 9.678723 & 0.3328 \\
        2048 & 127.853019 & 0.2016
    \end{tabular}

    \subsection*{\texttt{dgemm1}}
    \label{results1}
    \begin{tabular}{r r r r r} 
        \multicolumn{3}{c}{} & \multicolumn{2}{c}{\textit{compared to \texttt{dgemm0}}} \\
        $n$ & seconds & GFLOPS & speedup & max. difference \\
        \hline
        64 & 0.000892 & 0.8817 & 16\% & 0.000000 \\
        128 & 0.009744 & 0.6457 & 14\% & 0.000000 \\
        256 & 0.039746 & 1.2663 & 13\% & 0.000000 \\
        512 & 0.250638 & 1.6065 & 32\% & 0.000000 \\
        1024 & 6.884244 & 0.4679 & 29\% & 0.000000 \\
        2048 & 129.088407 & 0.1996 & $-1\%$ & 0.000000
    \end{tabular}
    
    \subsection*{\texttt{dgemm2}}
    \label{results2}
    \begin{tabular}{r r r r r} 
        \multicolumn{3}{c}{} & \multicolumn{2}{c}{\textit{compared to \texttt{dgemm0}}} \\
        $n$ & seconds & GFLOPS & speedup & max. difference \\
        \hline
        64 & 0.000333 & 2.3617 & 69\% & 0.000000 \\
        128 & 0.004979 & 1.2636 & 56\% & 0.000000 \\
        256 & 0.017665 & 2.8492 & 61\% & 0.000000 \\
        512 & 0.111821 & 3.6009 & 70\% & 0.000000 \\
        1024 & 2.942621 & 1.0947 & 70\% & 0.000001 \\
        2048 & 48.432833 & 0.5321 & 62\% & 0.000004
    \end{tabular}
    
    \subsection*{\texttt{dgemm3}}
    \label{results3}
    \begin{tabular}{r r r r r} 
        \multicolumn{3}{c}{} & \multicolumn{2}{c}{\textit{compared to \texttt{dgemm0}}} \\
        $n$ & seconds & GFLOPS & speedup & max. difference \\
        \hline
        64 & 0.000224 & 3.5109 & 79\% & 0.000000 \\
        128 & 0.001752 & 3.5910 & 85\% & 0.000000 \\
        256 & 0.004753 & 10.5894 & 90\% & 0.000000 \\
        512 & 0.039791 & 10.1192 & 89\% & 0.000000 \\
        1024 & 0.348491 & 9.2434 & 96\% & 0.000001 \\
        2048 & 3.395735 & 7.5889 & 97\% & 0.000004
    \end{tabular}
\end{center}

\section*{Conclusion}
These tests show that register reuse greatly impacts the runtime of square matrix multiplication
on modern hardware. Speed improvements of up to 97\% were obtained when multiplying $2048\times2048$
matrices of floating-point values by maximizing register reuse.
 
\end{document} 